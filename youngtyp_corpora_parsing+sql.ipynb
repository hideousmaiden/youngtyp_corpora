{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDUCQ5jEpgNV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "584d96d6-c0a6-4026-cb20-222b7db1360d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import re\n",
        "import sqlite3\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conn = sqlite3.connect('youngtyp.db')\n",
        "cur = conn.cursor()"
      ],
      "metadata": {
        "id": "wLimCZ3ap9pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# создаю пустую базу\n",
        "\n",
        "cur.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS texts\n",
        "(id_text text, text text)\n",
        "\"\"\")\n",
        "\n",
        "cur.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS meta\n",
        "(id_text text, author text, topic text, year int, affilation text) \n",
        "\"\"\")\n",
        "\n",
        "cur.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS sents \n",
        "(id_text text, id_sent int, sent text) \n",
        "\"\"\")\n",
        "\n",
        "cur.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS words\n",
        "(id_text text, id_sent int, id_word int, word text collate nocase, lemma text, pos text)\n",
        "\"\"\")\n",
        "cur.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS words_right\n",
        "(id_text text, id_sent int, id_word_right int, word_right text collate nocase, lemma_right text, pos_right text)\n",
        "\"\"\")\n",
        "\n",
        "cur.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS words_rright\n",
        "(id_text text, id_sent int, id_word_rright int, word_rright text collate nocase, lemma_rright text, pos_rright text)\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "conn.commit()  # создаю базу"
      ],
      "metadata": {
        "id": "l_JEILQyqHTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -r 'https://raw.githubusercontent.com/hideousmaiden/youngtyp_corpora/sasha/theses.json'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Dm3KngMIVYF",
        "outputId": "a555fc9a-b033-41cb-e05d-392f8943aceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-23 18:45:44--  https://raw.githubusercontent.com/hideousmaiden/youngtyp_corpora/sasha/theses.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3733358 (3.6M) [text/plain]\n",
            "Saving to: ‘raw.githubusercontent.com/hideousmaiden/youngtyp_corpora/sasha/theses.json’\n",
            "\n",
            "\r          raw.githu   0%[                    ]       0  --.-KB/s               \rraw.githubuserconte 100%[===================>]   3.56M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2022-10-23 18:45:44 (51.6 MB/s) - ‘raw.githubusercontent.com/hideousmaiden/youngtyp_corpora/sasha/theses.json’ saved [3733358/3733358]\n",
            "\n",
            "FINISHED --2022-10-23 18:45:44--\n",
            "Total wall clock time: 0.1s\n",
            "Downloaded: 1 files, 3.6M in 0.07s (51.6 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('raw.githubusercontent.com/hideousmaiden/youngtyp_corpora/sasha/theses.json', 'r', encoding=\"utf-8\") as f:\n",
        "  bodies = json.load(f)"
      ],
      "metadata": {
        "id": "GPx4QlNYKkHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# writing down raw text and sentences\n",
        "for t_id in bodies.keys():\n",
        "    cur.execute(\"\"\"INSERT INTO texts\n",
        "                (id_text, text) \n",
        "                VALUES (?, ?)\"\"\", (t_id, bodies[t_id]['text']))\n",
        "    cur.execute(\"\"\"INSERT INTO meta\n",
        "                (id_text, author, topic, year, affilation) \n",
        "                VALUES (?, ?, ?, ?, ?)\"\"\", (t_id,\n",
        "                                         bodies[t_id]['author'], bodies[t_id]['title'],\n",
        "                                         bodies[t_id]['year'], bodies[t_id]['affilation']))\n",
        "    sents = sent_tokenize(bodies[t_id]['text'])\n",
        "    for spair in enumerate(sents):\n",
        "      cur.execute(\"\"\"INSERT INTO sents\n",
        "                (id_text, id_sent, sent) \n",
        "                VALUES (?, ?, ?)\"\"\", (t_id, *spair))\n",
        "conn.commit()"
      ],
      "metadata": {
        "id": "nNp-xzERv6Xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iU2sfln5LwBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Парсинг"
      ],
      "metadata": {
        "id": "LJ_KTw4gzUhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install deeppavlov"
      ],
      "metadata": {
        "id": "_vOhU1OczX40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import deeppavlov\n",
        "#!python -m deeppavlov install morpho_ru_syntagrus_pymorphy"
      ],
      "metadata": {
        "id": "ZvxhvlrRzbsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from deeppavlov import build_model, configs\n",
        "\n",
        "#model = build_model(configs.morpho_tagger.UD2_0.morpho_ru_syntagrus_pymorphy, download=True)"
      ],
      "metadata": {
        "id": "BRcXD6Rzz3MP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer()"
      ],
      "metadata": {
        "id": "QEKR5KFG3k8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# writing down words\n",
        "\n",
        "cur.execute(\"\"\"\n",
        "    SELECT id_text, id_sent, sent\n",
        "    FROM sents\"\"\")\n",
        "sents = cur.fetchall()\n",
        "for one in sents:\n",
        "#    tags = list(filter(lambda x: len(x)>2, tags))\n",
        "    words = word_tokenize(one[-1])\n",
        "    words=[word for word in words if word.isalpha()]\n",
        "    if len(words) == 0:\n",
        "      continue\n",
        "    tags = model(words)\n",
        "    pos = list(map(lambda x: re.split('\\t',x)[2] if len(x)>2 else x, tags))\n",
        "    lems = list(map(lambda x: morph.parse(x)[0].normal_form, words))\n",
        "    for wrd in zip(range(len(pos)), words, lems, pos):\n",
        "        cur.execute(\"\"\"INSERT INTO words\n",
        "                (id_text, id_sent, id_word, word, lemma, pos) \n",
        "                VALUES (?, ?, ?, ?, ?, ?)\"\"\", (one[0], one[1], *wrd))\n",
        "        cur.execute(\"\"\"INSERT INTO words_right\n",
        "                (id_text, id_sent, id_word_right, word_right, lemma_right, pos_right) \n",
        "                VALUES (?, ?, ?, ?, ?, ?)\"\"\", (one[0], one[1], *wrd))\n",
        "        cur.execute(\"\"\"INSERT INTO words_rright\n",
        "                (id_text, id_sent, id_word_rright, word_rright, lemma_rright, pos_rright) \n",
        "                VALUES (?, ?, ?, ?, ?, ?)\"\"\", (one[0], one[1], *wrd))\n",
        "conn.commit()"
      ],
      "metadata": {
        "id": "YvXuvOB-5cWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Так как в задании эксплицитно указано, что в поиске будет до трёх слов, поиск нам показалось удобным осуществлять по таблице, где для каждого слова в отдельных колонках указаны его правые соседи и их свойства.\n",
        "\n",
        "\n",
        "\n",
        "> беда: SQL не может быть case-insensitive для русского"
      ],
      "metadata": {
        "id": "2P7O71DEdFHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_query(query, limit=10):\n",
        "  \"\"\"\n",
        "  process cql query\n",
        "\n",
        "  args:\n",
        "    query: str,\n",
        "  returns:\n",
        "    result: list of dict, list of dictionaries containing found results\n",
        "      pattern: str, found pattern\n",
        "      id_sent: int, \n",
        "      id_text: int,\n",
        "      id_word: int\n",
        "      (last_word_id: int)\n",
        "  \"\"\"\n",
        "# токены разделены словами\n",
        "  parts = query.split(' ')\n",
        "  # условия разделены плюсами\n",
        "  conds = list(map(lambda x: x.split('+'), parts))\n",
        "  pos_tr = {0:'pos',1:'pos_right', 2:'pos_rright'}\n",
        "  lemma_tr = {0:'lemma',1:'lemma_right', 2:'lemma_rright'}\n",
        "  word_tr = {0:'word',1:'word_right', 2:'word_rright'}\n",
        "  que = []\n",
        "  ask = {1:'''SELECT words.id_text, words.id_sent, words.id_word, words.word FROM words\n",
        "  JOIN words_right ON words_right.id_sent=words.id_sent AND words_right.id_text=words.id_text AND id_word_right = (id_word + 1)\n",
        "  JOIN words_rright ON words_rright.id_sent=words.id_sent AND words_rright.id_text=words.id_text AND id_word_rright = (id_word + 2)\n",
        "  WHERE {} COLLATE NOCASE\n",
        "  LIMIT {};''', 2:'''SELECT words.id_text, words.id_sent, words.id_word, words.word, id_word_right, word_right FROM words\n",
        "  JOIN words_right ON words_right.id_sent=words.id_sent AND words_right.id_text=words.id_text AND id_word_right = (id_word + 1)\n",
        "  JOIN words_rright ON words_rright.id_sent=words.id_sent AND words_rright.id_text=words.id_text AND id_word_rright = (id_word + 2)\n",
        "  WHERE {} COLLATE NOCASE\n",
        "  LIMIT {}\n",
        "  ;''', 3:'''SELECT words.id_text, words.id_sent, words.id_word, words.word, id_word_rright, word_rright FROM words\n",
        "  JOIN words_right ON words_right.id_sent=words.id_sent AND words_right.id_text=words.id_text AND id_word_right = (id_word + 1)\n",
        "  JOIN words_rright ON words_rright.id_sent=words.id_sent AND words_rright.id_text=words.id_text AND id_word_rright = (id_word + 2)\n",
        "  WHERE {} COLLATE NOCASE\n",
        "  LIMIT {}'''}\n",
        "  # по словам\n",
        "  for nn, ques in enumerate(conds):\n",
        "    for cond in ques:\n",
        "      #все условия для одного слова\n",
        "      if re.match('[A-Z]+', cond):\n",
        "          \n",
        "          que.append('{} = \"{}\"'.format(pos_tr[nn], cond))\n",
        "      elif re.match(r'\".+\"', cond):\n",
        "          que.append('(({} = {}) '.format(word_tr[nn], cond) + 'OR ({} = {}) '.format(word_tr[nn], cond.lower()) + 'OR ({} = {}))'.format(word_tr[nn], cond.title()))\n",
        "          #que.append('{} = {}'.format(word_tr[nn], cond))\n",
        "      else:\n",
        "          que.append('{} = \"{}\"'.format(lemma_tr[nn], morph.parse(cond)[0].normal_form))\n",
        "  big_que = ' AND '.join(que)\n",
        "  ask = ask[len(conds)].format(big_que, limit)\n",
        "  cur.execute(ask)\n",
        "  done = set(cur.fetchall())\n",
        "  result = []\n",
        "  for res in list(done):\n",
        "    dd = dict()\n",
        "    dd['pattern'] = ' '.join(list(filter(lambda x: str(x).isalpha(), res)))\n",
        "    dd['id_sent'] = res[1]\n",
        "    dd['id_text'] = res[0]\n",
        "    dd['id_word'] = res[2]\n",
        "    if len(res) > 4:\n",
        "      dd['last_word_id'] = res[-2]\n",
        "    result.append(dd)\n",
        "  return result"
      ],
      "metadata": {
        "id": "o_D6H4_yH9Dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_query(\"ADP NOUN\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CE4mwW9arRX",
        "outputId": "09b21890-eea1-474e-e350-7f7cc43a38d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'pattern': 'на предложения',\n",
              "  'id_sent': 3,\n",
              "  'id_text': 0,\n",
              "  'id_word': 8,\n",
              "  'last_word_id': 9},\n",
              " {'pattern': 'В описании',\n",
              "  'id_sent': 0,\n",
              "  'id_text': 1,\n",
              "  'id_word': 0,\n",
              "  'last_word_id': 1},\n",
              " {'pattern': 'на библиотеку',\n",
              "  'id_sent': 1,\n",
              "  'id_text': 1,\n",
              "  'id_word': 3,\n",
              "  'last_word_id': 4},\n",
              " {'pattern': 'из текстов',\n",
              "  'id_sent': 1,\n",
              "  'id_text': 0,\n",
              "  'id_word': 2,\n",
              "  'last_word_id': 3}]"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jq5a1ZFQa_XV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}